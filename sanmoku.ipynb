{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "board = np.zeros(9)\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_set(board,type = 1):\n",
    "    self_board = board\n",
    "    while(1):\n",
    "        index = np.random.randint(0,9)\n",
    "        if self_board[index] == 0:\n",
    "            self_board[index] = type\n",
    "            break\n",
    "    return self_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 1. 1. 2. 1. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    if i % 2 == 0:\n",
    "        board = random_set(board,2)\n",
    "    else:\n",
    "        board = random_set(board,1)\n",
    "\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'board = np.zeros(9)\\nbit_ary = [1,2,4,8,16,32,64,128,256]\\ncheck_ary = [7,56,448,73,146,292,273,84] \\nplayer1_operation = []\\nplayer2_operation = []\\n\\ndef checker(operation):\\n    for check in check_ary:\\n        if((check & operation) == check):\\n            return True\\n    return False\\n\\nfor i in range(9):\\n    if i % 2 == 0:\\n        board = random_set(board,2)\\n    else:\\n        board = random_set(board,1)\\n    \\n    if checker()*/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''board = np.zeros(9)\n",
    "bit_ary = [1,2,4,8,16,32,64,128,256]\n",
    "check_ary = [7,56,448,73,146,292,273,84] \n",
    "player1_operation = []\n",
    "player2_operation = []\n",
    "\n",
    "def checker(operation):\n",
    "    for check in check_ary:\n",
    "        if((check & operation) == check):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for i in range(9):\n",
    "    if i % 2 == 0:\n",
    "        board = random_set(board,2)\n",
    "    else:\n",
    "        board = random_set(board,1)\n",
    "    \n",
    "    if checker()*/'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sanmoku:\n",
    "    bit_ary = [1,2,4,8,16,32,64,128,256]\n",
    "    check_ary = [7,56,448,73,146,292,273,84] \n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros(9)\n",
    "        self.player1_operation = 0\n",
    "        self.player2_operation = 0\n",
    "\n",
    "    def checker(self, type = 1):\n",
    "        if type == 1:\n",
    "            operation = self.player1_operation\n",
    "        else:\n",
    "            operation = self.player2_operation\n",
    "        for check in self.check_ary:\n",
    "            if((check & operation) == check):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def random_set(self, type = 1):\n",
    "        if np.any(self.board == 0):\n",
    "            while True:\n",
    "                index = np.random.randint(0,9)\n",
    "                if self.board[index] == 0:\n",
    "                    break\n",
    "            self.board[index] = type\n",
    "            if type == 1:\n",
    "                self.player1_operation += self.bit_ary[index]\n",
    "            else:\n",
    "                self.player2_operation += self.bit_ary[index]\n",
    "\n",
    "    def set(self, index, type = 1):\n",
    "        self.board[index] = type\n",
    "        if type == 1:\n",
    "            self.player1_operation += self.bit_ary[index]\n",
    "        else:\n",
    "            self.player2_operation += self.bit_ary[index]\n",
    "\n",
    "    def step(self, action, type = 1):\n",
    "        self.set(action, type)\n",
    "        if type == 1:\n",
    "            un_type = 2\n",
    "        else:\n",
    "            un_type = 1\n",
    "        if self.checker(type) == False:\n",
    "            self.random_set(un_type)\n",
    "        return self.board, self.checker(type), self.checker(un_type)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros(9)\n",
    "        self.player1_operation = 0\n",
    "        self.player2_operation = 0\n",
    "        return self.board\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draw\n",
      "[[2. 1. 1.]\n",
      " [2. 1. 2.]\n",
      " [1. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "env = sanmoku()\n",
    "user = 1\n",
    "for i in range(9):\n",
    "    env.random_set(user)\n",
    "    if env.checker(user):\n",
    "        break\n",
    "    if user == 1:\n",
    "        user = 2\n",
    "    else:\n",
    "        user = 1\n",
    "if i == 8:\n",
    "    print(\"draw\")\n",
    "else:\n",
    "    print(\"winner:\" + str(user))\n",
    "print(env.board.reshape(3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "ETA = 0.5\n",
    "MAX_STEPS = 9\n",
    "NUM_EPISODES = 200\n",
    "NUM_REPETITION = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = Brain(num_states, num_actions)\n",
    "\n",
    "    def update_Q_function(self, observation, action, reward, observation_next):\n",
    "        self.brain.update_Q_table(\n",
    "            observation, action, reward, observation_next)\n",
    "\n",
    "    def get_action(self, observation, step):\n",
    "        action = self.brain.decide_action(observation, step)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(num_states, num_actions))\n",
    "\n",
    "    def observation_to_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "\n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        state = self.observation_to_state(observation)\n",
    "        state_next = self.observation_to_state(observation_next)\n",
    "        Max_Q_next = max(self.q_table[state_next][:])\n",
    "        self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        state = self.observation_to_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.deduplication(observation, self.q_table[state][:]))\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def deduplication(self, observation, q_table):\n",
    "        bool_not_observation = np.logical_not(observation > 0)\n",
    "        mask = bool_not_observation.astype(int)\n",
    "        return q_table * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = sanmoku()\n",
    "        num_states = 3**9\n",
    "        num_actions = 9\n",
    "\n",
    "        self.num_win = 0\n",
    "        self.num_lose = 0\n",
    "        \n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def run(self):\n",
    "        complete_episodes = 0\n",
    "        is_episode_final = False\n",
    "\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation = self.env.reset()\n",
    "\n",
    "            for step in range(MAX_STEPS):\n",
    "                action = self.agent.get_action(observation, episode)\n",
    "                observation_next, done, un_done = self.env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    reward = 1\n",
    "                    complete_episodes += 1\n",
    "                    self.num_win += 1\n",
    "                elif un_done:\n",
    "                    reward = -1\n",
    "                    complete_episodes = 0\n",
    "                    self.num_lose += 1\n",
    "                else:\n",
    "                    reward = 0\n",
    "\n",
    "                self.agent.update_Q_function(observation, action, reward, observation_next)\n",
    "\n",
    "                observation = observation_next\n",
    "\n",
    "                if done:\n",
    "                    print('{0} Episode : AI won and Finished after {1} time steps'.format(episode, step + 1))\n",
    "                    break\n",
    "                if un_done:\n",
    "                    print('{0} Episode : AI lost and Finished after {1} time steps'.format(episode, step + 1))\n",
    "                    break\n",
    "\n",
    "            if not done or un_done:\n",
    "                print('{0} Episode : Draw'.format(episode))\n",
    "            if complete_episodes >= 10:\n",
    "                print(\"10連続勝利\")\n",
    "                is_episode_final = True\n",
    "\n",
    "            if is_episode_final is True:\n",
    "                break\n",
    "        \n",
    "        print(\"Win rate:{0}\".format(self.num_win / (self.num_win + self.num_lose)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode : AI lost and Finished after 5 time steps\n",
      "0 Episode : Draw\n",
      "1 Episode : AI lost and Finished after 4 time steps\n",
      "1 Episode : Draw\n",
      "2 Episode : AI won and Finished after 4 time steps\n",
      "3 Episode : AI won and Finished after 6 time steps\n",
      "4 Episode : AI won and Finished after 5 time steps\n",
      "5 Episode : AI lost and Finished after 4 time steps\n",
      "5 Episode : Draw\n",
      "6 Episode : AI won and Finished after 5 time steps\n",
      "7 Episode : AI won and Finished after 4 time steps\n",
      "8 Episode : AI won and Finished after 4 time steps\n",
      "9 Episode : AI won and Finished after 6 time steps\n",
      "10 Episode : AI won and Finished after 4 time steps\n",
      "11 Episode : AI won and Finished after 4 time steps\n",
      "12 Episode : AI won and Finished after 5 time steps\n",
      "13 Episode : AI lost and Finished after 3 time steps\n",
      "13 Episode : Draw\n",
      "14 Episode : AI won and Finished after 4 time steps\n",
      "15 Episode : AI lost and Finished after 4 time steps\n",
      "15 Episode : Draw\n",
      "16 Episode : AI lost and Finished after 3 time steps\n",
      "16 Episode : Draw\n",
      "17 Episode : AI won and Finished after 6 time steps\n",
      "18 Episode : AI won and Finished after 5 time steps\n",
      "19 Episode : AI won and Finished after 4 time steps\n",
      "20 Episode : AI won and Finished after 4 time steps\n",
      "21 Episode : AI won and Finished after 4 time steps\n",
      "22 Episode : AI won and Finished after 4 time steps\n",
      "23 Episode : AI won and Finished after 5 time steps\n",
      "24 Episode : AI lost and Finished after 4 time steps\n",
      "24 Episode : Draw\n",
      "25 Episode : AI won and Finished after 3 time steps\n",
      "26 Episode : AI won and Finished after 6 time steps\n",
      "27 Episode : AI won and Finished after 5 time steps\n",
      "28 Episode : AI won and Finished after 4 time steps\n",
      "29 Episode : AI won and Finished after 3 time steps\n",
      "30 Episode : AI won and Finished after 3 time steps\n",
      "31 Episode : AI won and Finished after 5 time steps\n",
      "32 Episode : AI won and Finished after 7 time steps\n",
      "33 Episode : AI won and Finished after 4 time steps\n",
      "34 Episode : AI lost and Finished after 4 time steps\n",
      "34 Episode : Draw\n",
      "35 Episode : AI won and Finished after 4 time steps\n",
      "36 Episode : AI won and Finished after 3 time steps\n",
      "37 Episode : AI won and Finished after 7 time steps\n",
      "38 Episode : AI won and Finished after 4 time steps\n",
      "39 Episode : AI won and Finished after 6 time steps\n",
      "40 Episode : AI lost and Finished after 4 time steps\n",
      "40 Episode : Draw\n",
      "41 Episode : AI won and Finished after 5 time steps\n",
      "42 Episode : AI won and Finished after 4 time steps\n",
      "43 Episode : AI lost and Finished after 4 time steps\n",
      "43 Episode : Draw\n",
      "44 Episode : AI won and Finished after 5 time steps\n",
      "45 Episode : AI won and Finished after 5 time steps\n",
      "46 Episode : AI won and Finished after 5 time steps\n",
      "47 Episode : AI won and Finished after 4 time steps\n",
      "48 Episode : AI lost and Finished after 4 time steps\n",
      "48 Episode : Draw\n",
      "49 Episode : AI won and Finished after 5 time steps\n",
      "50 Episode : AI won and Finished after 5 time steps\n",
      "51 Episode : AI won and Finished after 3 time steps\n",
      "52 Episode : AI won and Finished after 4 time steps\n",
      "53 Episode : AI lost and Finished after 4 time steps\n",
      "53 Episode : Draw\n",
      "54 Episode : AI won and Finished after 7 time steps\n",
      "55 Episode : AI won and Finished after 4 time steps\n",
      "56 Episode : AI won and Finished after 5 time steps\n",
      "57 Episode : AI lost and Finished after 3 time steps\n",
      "57 Episode : Draw\n",
      "58 Episode : AI won and Finished after 5 time steps\n",
      "59 Episode : AI won and Finished after 5 time steps\n",
      "60 Episode : AI lost and Finished after 3 time steps\n",
      "60 Episode : Draw\n",
      "61 Episode : AI won and Finished after 4 time steps\n",
      "62 Episode : AI won and Finished after 3 time steps\n",
      "63 Episode : AI won and Finished after 4 time steps\n",
      "64 Episode : AI lost and Finished after 4 time steps\n",
      "64 Episode : Draw\n",
      "65 Episode : AI won and Finished after 4 time steps\n",
      "66 Episode : AI won and Finished after 7 time steps\n",
      "67 Episode : AI won and Finished after 5 time steps\n",
      "68 Episode : AI won and Finished after 5 time steps\n",
      "69 Episode : AI won and Finished after 4 time steps\n",
      "70 Episode : AI won and Finished after 5 time steps\n",
      "71 Episode : AI won and Finished after 3 time steps\n",
      "72 Episode : AI lost and Finished after 4 time steps\n",
      "72 Episode : Draw\n",
      "73 Episode : AI won and Finished after 3 time steps\n",
      "74 Episode : AI lost and Finished after 3 time steps\n",
      "74 Episode : Draw\n",
      "75 Episode : AI won and Finished after 6 time steps\n",
      "76 Episode : AI lost and Finished after 4 time steps\n",
      "76 Episode : Draw\n",
      "77 Episode : AI won and Finished after 4 time steps\n",
      "78 Episode : AI lost and Finished after 4 time steps\n",
      "78 Episode : Draw\n",
      "79 Episode : AI won and Finished after 3 time steps\n",
      "80 Episode : AI won and Finished after 7 time steps\n",
      "81 Episode : AI won and Finished after 4 time steps\n",
      "82 Episode : AI won and Finished after 5 time steps\n",
      "83 Episode : AI won and Finished after 4 time steps\n",
      "84 Episode : AI won and Finished after 4 time steps\n",
      "85 Episode : AI won and Finished after 4 time steps\n",
      "86 Episode : AI won and Finished after 3 time steps\n",
      "87 Episode : AI won and Finished after 6 time steps\n",
      "88 Episode : AI lost and Finished after 3 time steps\n",
      "88 Episode : Draw\n",
      "89 Episode : AI won and Finished after 6 time steps\n",
      "90 Episode : AI won and Finished after 7 time steps\n",
      "91 Episode : AI won and Finished after 6 time steps\n",
      "92 Episode : AI won and Finished after 4 time steps\n",
      "93 Episode : AI won and Finished after 3 time steps\n",
      "94 Episode : AI won and Finished after 4 time steps\n",
      "95 Episode : AI won and Finished after 5 time steps\n",
      "96 Episode : AI won and Finished after 4 time steps\n",
      "97 Episode : AI won and Finished after 6 time steps\n",
      "98 Episode : AI won and Finished after 4 time steps\n",
      "10連続勝利\n",
      "Win rate:0.797979797979798\n"
     ]
    }
   ],
   "source": [
    "sanmoku_env = Environment()\n",
    "sanmoku_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(num_states, num_actions))\n",
    "\n",
    "    def observation_to_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "\n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        state = self.observation_to_state(observation)\n",
    "        state_next = self.observation_to_state(observation_next)\n",
    "        Max_Q_next = max(self.deduplication(observation, self.q_table[state_next][:]))\n",
    "        self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        state = self.observation_to_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.deduplication(observation, self.q_table[state][:]))\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def deduplication(self, observation, q_table):\n",
    "        bool_not_observation = np.logical_not(observation > 0)\n",
    "        mask = bool_not_observation.astype(int)\n",
    "        return q_table * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode : AI lost and Finished after 4 time steps\n",
      "0 Episode : Draw\n",
      "1 Episode : AI won and Finished after 3 time steps\n",
      "2 Episode : AI won and Finished after 6 time steps\n",
      "3 Episode : AI won and Finished after 5 time steps\n",
      "4 Episode : AI lost and Finished after 3 time steps\n",
      "4 Episode : Draw\n",
      "5 Episode : AI won and Finished after 4 time steps\n",
      "6 Episode : AI won and Finished after 5 time steps\n",
      "7 Episode : AI won and Finished after 3 time steps\n",
      "8 Episode : AI lost and Finished after 3 time steps\n",
      "8 Episode : Draw\n",
      "9 Episode : AI lost and Finished after 3 time steps\n",
      "9 Episode : Draw\n",
      "10 Episode : AI won and Finished after 6 time steps\n",
      "11 Episode : AI won and Finished after 5 time steps\n",
      "12 Episode : AI lost and Finished after 3 time steps\n",
      "12 Episode : Draw\n",
      "13 Episode : AI lost and Finished after 3 time steps\n",
      "13 Episode : Draw\n",
      "14 Episode : AI won and Finished after 5 time steps\n",
      "15 Episode : AI lost and Finished after 3 time steps\n",
      "15 Episode : Draw\n",
      "16 Episode : AI lost and Finished after 3 time steps\n",
      "16 Episode : Draw\n",
      "17 Episode : AI lost and Finished after 4 time steps\n",
      "17 Episode : Draw\n",
      "18 Episode : AI lost and Finished after 4 time steps\n",
      "18 Episode : Draw\n",
      "19 Episode : AI lost and Finished after 4 time steps\n",
      "19 Episode : Draw\n",
      "20 Episode : AI won and Finished after 4 time steps\n",
      "21 Episode : AI lost and Finished after 4 time steps\n",
      "21 Episode : Draw\n",
      "22 Episode : AI lost and Finished after 4 time steps\n",
      "22 Episode : Draw\n",
      "23 Episode : AI won and Finished after 5 time steps\n",
      "24 Episode : AI won and Finished after 6 time steps\n",
      "25 Episode : AI won and Finished after 5 time steps\n",
      "26 Episode : AI lost and Finished after 3 time steps\n",
      "26 Episode : Draw\n",
      "27 Episode : AI won and Finished after 6 time steps\n",
      "28 Episode : AI lost and Finished after 4 time steps\n",
      "28 Episode : Draw\n",
      "29 Episode : AI lost and Finished after 3 time steps\n",
      "29 Episode : Draw\n",
      "30 Episode : AI won and Finished after 5 time steps\n",
      "31 Episode : AI won and Finished after 4 time steps\n",
      "32 Episode : AI lost and Finished after 3 time steps\n",
      "32 Episode : Draw\n",
      "33 Episode : AI lost and Finished after 3 time steps\n",
      "33 Episode : Draw\n",
      "34 Episode : AI won and Finished after 6 time steps\n",
      "35 Episode : AI won and Finished after 6 time steps\n",
      "36 Episode : AI won and Finished after 5 time steps\n",
      "37 Episode : AI lost and Finished after 4 time steps\n",
      "37 Episode : Draw\n",
      "38 Episode : AI won and Finished after 5 time steps\n",
      "39 Episode : AI lost and Finished after 4 time steps\n",
      "39 Episode : Draw\n",
      "40 Episode : AI lost and Finished after 3 time steps\n",
      "40 Episode : Draw\n",
      "41 Episode : AI lost and Finished after 4 time steps\n",
      "41 Episode : Draw\n",
      "42 Episode : AI won and Finished after 6 time steps\n",
      "43 Episode : AI lost and Finished after 4 time steps\n",
      "43 Episode : Draw\n",
      "44 Episode : AI won and Finished after 5 time steps\n",
      "45 Episode : AI won and Finished after 6 time steps\n",
      "46 Episode : AI won and Finished after 4 time steps\n",
      "47 Episode : AI lost and Finished after 4 time steps\n",
      "47 Episode : Draw\n",
      "48 Episode : AI lost and Finished after 4 time steps\n",
      "48 Episode : Draw\n",
      "49 Episode : AI won and Finished after 6 time steps\n",
      "Win rate:0.5\n"
     ]
    }
   ],
   "source": [
    "sanmoku_env2 = Environment()\n",
    "sanmoku_env2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanmoku勝率\n",
      "平均:0.67792,中央値:0.67799,標準偏差:0.00440,最大:0.71371,最小:0.66802\n",
      "sanmoku2勝率\n",
      "平均:0.68571,中央値:0.68489,標準偏差:0.00498,最大:0.75159,最小:0.67458\n",
      "sanmoku3勝率\n",
      "平均:0.68231,中央値:0.68241,標準偏差:0.00650,最大:0.69059,最小:0.59873\n",
      "sanmokuエピソード数\n",
      "平均:98.14200,中央値:80.50000,標準偏差:68.26769,最大:199.00000,最小:9.00000\n",
      "sanmoku2エピソード数\n",
      "平均:96.43600,中央値:78.00000,標準偏差:67.89677,最大:199.00000,最小:9.00000\n",
      "sanmoku3エピソード数\n",
      "平均:92.71500,中央値:73.50000,標準偏差:67.61171,最大:199.00000,最小:9.00000\n"
     ]
    }
   ],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = sanmoku()\n",
    "        num_states = 3**9\n",
    "        num_actions = 9\n",
    "\n",
    "        self.num_win = 0\n",
    "        self.num_lose = 0\n",
    "        \n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env = sanmoku()\n",
    "        num_states = 3**9\n",
    "        num_actions = 9\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def run(self):\n",
    "        complete_episodes = 0\n",
    "        is_episode_final = False\n",
    "\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation = self.env.reset()\n",
    "\n",
    "            for step in range(MAX_STEPS):\n",
    "                action = self.agent.get_action(observation, episode)\n",
    "                observation_next, done, un_done = self.env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    reward = 1\n",
    "                    complete_episodes += 1\n",
    "                    self.num_win += 1\n",
    "                elif un_done:\n",
    "                    reward = -1\n",
    "                    complete_episodes = 0\n",
    "                    self.num_lose += 1\n",
    "                else:\n",
    "                    reward = 0\n",
    "\n",
    "                self.agent.update_Q_function(observation, action, reward, observation_next)\n",
    "\n",
    "                observation = observation_next\n",
    "\n",
    "                if done or un_done:\n",
    "                    break\n",
    "\n",
    "            if complete_episodes >= 10:\n",
    "                is_episode_final = True\n",
    "\n",
    "            if is_episode_final is True:\n",
    "                break\n",
    "        \n",
    "        win_rate = self.num_win / (self.num_win + self.num_lose)\n",
    "        return win_rate, episode\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(num_states, num_actions))\n",
    "\n",
    "    def observation_to_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "\n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        state = self.observation_to_state(observation)\n",
    "        state_next = self.observation_to_state(observation_next)\n",
    "        Max_Q_next = max(self.q_table[state_next][:])\n",
    "        self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        state = self.observation_to_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.deduplication(observation, self.q_table[state][:]))\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def deduplication(self, observation, q_table):\n",
    "        bool_not_observation = np.logical_not(observation > 0)\n",
    "        mask = bool_not_observation.astype(int)\n",
    "        return q_table * mask\n",
    "\n",
    "sanmoku_env = Environment()\n",
    "sanmoku_env_win_rate = np.array([])\n",
    "sanmoku_env_episode = np.array([])\n",
    "for i in range(NUM_REPETITION):\n",
    "    sanmoku_env.reset()\n",
    "    win_rate, episode = sanmoku_env.run()\n",
    "    sanmoku_env_win_rate = np.append(sanmoku_env_win_rate, win_rate)\n",
    "    sanmoku_env_episode = np.append(sanmoku_env_episode, episode)\n",
    "    \n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(num_states, num_actions))\n",
    "\n",
    "    def observation_to_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "\n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        state = self.observation_to_state(observation)\n",
    "        state_next = self.observation_to_state(observation_next)\n",
    "        Max_Q_next = max(self.deduplication(observation, self.q_table[state_next][:]))\n",
    "        self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        state = self.observation_to_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.deduplication(observation, self.q_table[state][:]))\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def deduplication(self, observation, q_table):\n",
    "        bool_not_observation = np.logical_not(observation > 0)\n",
    "        mask = bool_not_observation.astype(int)\n",
    "        return q_table * mask\n",
    "\n",
    "sanmoku_env2 = Environment()\n",
    "sanmoku_env2_win_rate = np.array([])\n",
    "sanmoku_env2_episode = np.array([])\n",
    "for i in range(NUM_REPETITION):\n",
    "    sanmoku_env2.reset()\n",
    "    win_rate, episode = sanmoku_env2.run()\n",
    "    sanmoku_env2_win_rate = np.append(sanmoku_env2_win_rate, win_rate)\n",
    "    sanmoku_env2_episode = np.append(sanmoku_env2_episode, episode)\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(num_states, num_actions))\n",
    "\n",
    "    def observation_to_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "\n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        state = self.observation_to_state(observation)\n",
    "        state_next = self.observation_to_state(observation_next)\n",
    "        Max_Q_next = max(self.deduplication(observation, self.q_table[state_next][:]))\n",
    "        if reward == 0:\n",
    "            self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "                ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "        else:\n",
    "            self.q_table[state, action] = self.q_table[state, action] + ETA * (reward - self.q_table[state, action])\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        state = self.observation_to_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.deduplication(observation, self.q_table[state][:]))\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def deduplication(self, observation, q_table):\n",
    "        bool_not_observation = np.logical_not(observation > 0)\n",
    "        mask = bool_not_observation.astype(int)\n",
    "        return q_table * mask\n",
    "\n",
    "sanmoku_env3 = Environment()\n",
    "sanmoku_env3_win_rate = np.array([])\n",
    "sanmoku_env3_episode = np.array([])\n",
    "for i in range(NUM_REPETITION):\n",
    "    sanmoku_env3.reset()\n",
    "    win_rate, episode = sanmoku_env3.run()\n",
    "    sanmoku_env3_win_rate = np.append(sanmoku_env3_win_rate, win_rate)\n",
    "    sanmoku_env3_episode = np.append(sanmoku_env3_episode, episode)\n",
    "\n",
    "sanmoku_env_win_rate_mean = np.mean(sanmoku_env_win_rate)\n",
    "sanmoku_env_win_rate_med = np.median(sanmoku_env_win_rate)\n",
    "sanmoku_env_win_rate_std = np.std(sanmoku_env_win_rate)\n",
    "sanmoku_env_episode_mean = np.mean(sanmoku_env_episode)\n",
    "sanmoku_env_episode_med = np.median(sanmoku_env_episode)\n",
    "sanmoku_env_episode_std = np.std(sanmoku_env_episode)\n",
    "sanmoku_env2_win_rate_mean = np.mean(sanmoku_env2_win_rate)\n",
    "sanmoku_env2_win_rate_med = np.median(sanmoku_env2_win_rate)\n",
    "sanmoku_env2_win_rate_std = np.std(sanmoku_env2_win_rate)\n",
    "sanmoku_env2_episode_mean = np.mean(sanmoku_env2_episode)\n",
    "sanmoku_env2_episode_med = np.median(sanmoku_env2_episode)\n",
    "sanmoku_env2_episode_std = np.std(sanmoku_env2_episode)\n",
    "sanmoku_env3_win_rate_mean = np.mean(sanmoku_env3_win_rate)\n",
    "sanmoku_env3_win_rate_med = np.median(sanmoku_env3_win_rate)\n",
    "sanmoku_env3_win_rate_std = np.std(sanmoku_env3_win_rate)\n",
    "sanmoku_env3_episode_mean = np.mean(sanmoku_env3_episode)\n",
    "sanmoku_env3_episode_med = np.median(sanmoku_env3_episode)\n",
    "sanmoku_env3_episode_std = np.std(sanmoku_env3_episode)\n",
    "\n",
    "print(\"sanmoku勝率\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env_win_rate_mean,sanmoku_env_win_rate_med,sanmoku_env_win_rate_std,max(sanmoku_env_win_rate),min(sanmoku_env_win_rate)))\n",
    "print(\"sanmoku2勝率\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env2_win_rate_mean,sanmoku_env2_win_rate_med,sanmoku_env2_win_rate_std,max(sanmoku_env2_win_rate),min(sanmoku_env2_win_rate)))\n",
    "print(\"sanmoku3勝率\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env3_win_rate_mean,sanmoku_env3_win_rate_med,sanmoku_env3_win_rate_std,max(sanmoku_env3_win_rate),min(sanmoku_env3_win_rate)))\n",
    "print(\"sanmokuエピソード数\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env_episode_mean,sanmoku_env_episode_med,sanmoku_env_episode_std,max(sanmoku_env_episode),min(sanmoku_env_episode)))\n",
    "print(\"sanmoku2エピソード数\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env2_episode_mean,sanmoku_env2_episode_med,sanmoku_env2_episode_std,max(sanmoku_env2_episode),min(sanmoku_env2_episode)))\n",
    "print(\"sanmoku3エピソード数\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env3_episode_mean,sanmoku_env3_episode_med,sanmoku_env3_episode_std,max(sanmoku_env3_episode),min(sanmoku_env3_episode)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanmoku2勝率\n",
      "平均:0.91803,中央値:0.92389,標準偏差:0.01842,最大:0.93152,最小:0.71147\n",
      "sanmoku2エピソード数\n",
      "平均:11.79140,中央値:10.00000,標準偏差:4.71893,最大:176.00000,最小:9.00000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "NUM_REPETITION = 20000\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = sanmoku()\n",
    "        num_states = 3**9\n",
    "        num_actions = 9\n",
    "\n",
    "        self.num_win = 0\n",
    "        self.num_lose = 0\n",
    "        \n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env = sanmoku()\n",
    "        num_states = 3**9\n",
    "        num_actions = 9\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def run(self):\n",
    "        complete_episodes = 0\n",
    "        is_episode_final = False\n",
    "\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation = copy.deepcopy(self.env.reset())\n",
    "            type = random.randint(1,2)\n",
    "\n",
    "            for step in range(MAX_STEPS):\n",
    "                action = self.agent.get_action(observation, episode)\n",
    "                observation_next, done, un_done = self.env.step(action,type)\n",
    "\n",
    "                if done:\n",
    "                    reward = 1\n",
    "                    complete_episodes += 1\n",
    "                    self.num_win += 1\n",
    "                elif un_done:\n",
    "                    reward = -1\n",
    "                    complete_episodes = 0\n",
    "                    self.num_lose += 1\n",
    "                else:\n",
    "                    reward = 0\n",
    "\n",
    "                self.agent.update_Q_function(observation, action, reward, observation_next)\n",
    "\n",
    "                observation = copy.deepcopy(observation_next)\n",
    "\n",
    "                if done or un_done:\n",
    "                    break\n",
    "\n",
    "            if complete_episodes >= 10:\n",
    "                is_episode_final = True\n",
    "\n",
    "            if is_episode_final is True:\n",
    "                break\n",
    "        \n",
    "        win_rate = self.num_win / (self.num_win + self.num_lose)\n",
    "        return win_rate, episode\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(num_states, num_actions))\n",
    "\n",
    "    def observation_to_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "\n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        state = self.observation_to_state(observation)\n",
    "        state_next = self.observation_to_state(observation_next)\n",
    "        Max_Q_next = max(self.deduplication(observation, self.q_table[state_next][:]))\n",
    "        self.q_table[state, action] = self.q_table[state, action] + \\\n",
    "            ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        state = self.observation_to_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.deduplication(observation, self.q_table[state][:]))\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def deduplication(self, observation, q_table):\n",
    "        bool_not_observation = np.logical_not(observation > 0)\n",
    "        mask = bool_not_observation.astype(int)\n",
    "        return q_table * mask\n",
    "\n",
    "sanmoku_env2 = Environment()\n",
    "sanmoku_env2_win_rate = np.array([])\n",
    "sanmoku_env2_episode = np.array([])\n",
    "for i in range(NUM_REPETITION):\n",
    "    win_rate, episode = sanmoku_env2.run()\n",
    "    sanmoku_env2_win_rate = np.append(sanmoku_env2_win_rate, win_rate)\n",
    "    sanmoku_env2_episode = np.append(sanmoku_env2_episode, episode)\n",
    "\n",
    "sanmoku_env2_win_rate_mean = np.mean(sanmoku_env2_win_rate)\n",
    "sanmoku_env2_win_rate_med = np.median(sanmoku_env2_win_rate)\n",
    "sanmoku_env2_win_rate_std = np.std(sanmoku_env2_win_rate)\n",
    "sanmoku_env2_episode_mean = np.mean(sanmoku_env2_episode)\n",
    "sanmoku_env2_episode_med = np.median(sanmoku_env2_episode)\n",
    "sanmoku_env2_episode_std = np.std(sanmoku_env2_episode)\n",
    "\n",
    "print(\"sanmoku2勝率\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env2_win_rate_mean,sanmoku_env2_win_rate_med,sanmoku_env2_win_rate_std,max(sanmoku_env2_win_rate),min(sanmoku_env2_win_rate)))\n",
    "print(\"sanmoku2エピソード数\")\n",
    "print(\"平均:{0:.05f},中央値:{1:.05f},標準偏差:{2:.05f},最大:{3:.05f},最小:{4:.05f}\".format(sanmoku_env2_episode_mean,sanmoku_env2_episode_med,sanmoku_env2_episode_std,max(sanmoku_env2_episode),min(sanmoku_env2_episode)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ゲームスタート!\n",
      "[['0' '1' '2']\n",
      " ['3' '4' '5']\n",
      " ['6' '7' '8']]\n",
      "あなたのターン\n",
      "[['︎' '1' '2']\n",
      " ['3' '4' '5']\n",
      " ['6' '7' '8']]\n",
      "AIのターン\n",
      "[['︎' '1' '2']\n",
      " ['3' '×' '5']\n",
      " ['6' '7' '8']]\n",
      "あなたのターン\n",
      "[['︎' '1' '2']\n",
      " ['3' '×' '5']\n",
      " ['6' '7' '︎']]\n",
      "AIのターン\n",
      "[['︎' '×' '2']\n",
      " ['3' '×' '5']\n",
      " ['6' '7' '︎']]\n",
      "あなたのターン\n",
      "[['︎' '×' '2']\n",
      " ['3' '×' '︎']\n",
      " ['6' '7' '︎']]\n",
      "AIのターン\n",
      "[['︎' '×' '2']\n",
      " ['3' '×' '︎']\n",
      " ['6' '×' '︎']]\n",
      "AIの勝ち\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class VS_Q_Learning:\n",
    "    \n",
    "    def __init__(self):\n",
    "        num_states = 9\n",
    "        num_actions = 9\n",
    "        self.game_board = np.zeros(num_states)\n",
    "        self.agent = VS_Agent(num_states, num_actions)\n",
    "\n",
    "    def play(self):\n",
    "        \n",
    "        pre = int(input(\"先攻→0, 後攻→1を入力してください\"))\n",
    "        print(\"ゲームスタート!\")\n",
    "        self.show()\n",
    "        #observation = copy.deepcopy(self.game_board)\n",
    "        winner = 0\n",
    "            \n",
    "        for step in range(9):\n",
    "            \n",
    "            if pre == 0:\n",
    "                winner = self.your_turn()\n",
    "                if (winner == 0 and step < 4):\n",
    "                    winner = self.q_learning_turn()\n",
    "            else:\n",
    "                winner = self.q_learning_turn()\n",
    "                if (winner == 0 and step < 4):\n",
    "                    winner = self.your_turn()\n",
    "\n",
    "            if winner == 1:\n",
    "                print(\"AIの勝ち\")\n",
    "                break\n",
    "            elif winner == 2:\n",
    "                print(\"あなたの勝ち\")\n",
    "                break\n",
    "\n",
    "            if step == 4:\n",
    "                print(\"引き分け\")\n",
    "                complete_episodes = 0\n",
    "                break\n",
    "\n",
    "    def your_turn(self):\n",
    "        print(\"あなたのターン\")\n",
    "        pos = int(input(\"手を選んでください:\"))\n",
    "        self.game_board[pos] = 2\n",
    "        self.show()\n",
    "        winner = self.winner_func(2)\n",
    "        return winner\n",
    "        \n",
    "    def q_learning_turn(self):\n",
    "        print(\"AIのターン\")\n",
    "        action = self.agent.get_action(self.game_board)\n",
    "        self.game_board[action] = 1\n",
    "        self.show()\n",
    "        winner = self.winner_func(1)\n",
    "        return winner\n",
    "\n",
    "    def winner_func(self, player):\n",
    "        # 勝ち手を列挙\n",
    "        lines = [\n",
    "        [0, 1, 2],\n",
    "        [3, 4, 5],\n",
    "        [6, 7, 8],\n",
    "        [0, 3, 6],\n",
    "        [1, 4, 7],\n",
    "        [2, 5, 8],\n",
    "        [0, 4, 8],\n",
    "        [2, 4, 6],\n",
    "        ]\n",
    "        for i in range(0, len(lines)):\n",
    "            [a, b, c] = lines[i]\n",
    "            \n",
    "            if self.game_board[a] and self.game_board[a] == self.game_board[b] and self.game_board[a] == self.game_board[c]: \n",
    "                return player\n",
    "        return 0\n",
    "\n",
    "    def show(self):\n",
    "        show_board = np.array([\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"])\n",
    "        for i in range(9):\n",
    "            x = self.game_board[i]\n",
    "            if (x==0):\n",
    "                show_board[i] = str(i)\n",
    "            elif (x==1):\n",
    "                show_board[i] = \"×\"\n",
    "            else:\n",
    "                show_board[i] = \"︎O\"\n",
    "        board = np.reshape(show_board, [3,3])\n",
    "        print(board)\n",
    "\n",
    "class VS_Agent:\n",
    "    '''三目並べのエージェントクラス'''\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = VS_Brain(num_states, num_actions)\n",
    "        # エージェントが行動を決定するための頭脳を生成\n",
    "            \n",
    "    def get_action(self, observation):\n",
    "        '''行動の決定'''\n",
    "        action = self.brain.decide_action(observation)\n",
    "        return action\n",
    "\n",
    "class VS_Brain:\n",
    "    '''エージェントが持つ脳となるクラスです。Q学習を実行します'''\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def digitize_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "        \n",
    "    def decide_action(self, observation):\n",
    "        choices = []\n",
    "        for i in range(9):\n",
    "            if (observation[i] == 0):\n",
    "                choices.append(i)\n",
    "        state = self.digitize_state(observation)\n",
    "        action = np.argmax(q_table[state][:])\n",
    "        if (action not in choices):\n",
    "            action = random.choice(choices)\n",
    "        return action\n",
    "\n",
    "q_table = sanmoku_env2.agent.brain.q_table\n",
    "game = VS_Q_Learning()\n",
    "game.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a01296a933a7f1455ff41e0024d3cc6ec3d56a277d2de426f27cc1ef10ac7265"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rl_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
